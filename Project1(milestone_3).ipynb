{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM27xL3HsLuRre1O1CKNyBz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvirA25/Ariel_Segmentation_Project/blob/milestone-3/Project1(milestone_3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pvQ1LMV2Z-q"
      },
      "outputs": [],
      "source": [
        "!pip install patchify\n",
        "!pip install keras\n",
        "!pip install segmentation_models\n",
        "!pip install tensorflow==2.7.0\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from IPython.display import SVG\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os, re, sys, random, shutil, cv2\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from matplotlib import pyplot as plt\n",
        "from patchify import patchify\n",
        "from PIL import Image\n",
        "import segmentation_models as sm\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "scaler = MinMaxScaler()\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, Nadam\n",
        "from tensorflow.keras import applications, optimizers\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.utils import model_to_dot, plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, ZeroPadding2D, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6tRktSgY2iJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob as gb\n",
        "from google.colab import drive\n",
        "# below storing the dataset that we had from our drive into root_directory \n",
        "root_directory = '/content/drive/MyDrive/Semantic segmentation dataset'\n",
        "patch_size = 256"
      ],
      "metadata": {
        "id": "NUjoxOyZ2kz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patchifying image\n",
        "\n",
        "image_dataset = []  \n",
        "for path, subdirs, files in os.walk(root_directory):\n",
        "    #print(path)  \n",
        "    dirname = path.split(os.path.sep)[-1]\n",
        "    if dirname == 'images':   #Find all 'images' directories\n",
        "        images = os.listdir(path)  #List of all image names in this subdirectory\n",
        "        for i, image_name in enumerate(images):  \n",
        "            if image_name.endswith(\".jpg\"):   #Only read jpg images...\n",
        "               \n",
        "                image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n",
        "                SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
        "                SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
        "                image = Image.fromarray(image)\n",
        "                image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
        "                #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
        "                image = np.array(image)             \n",
        "       \n",
        "                #Extract patches from each image\n",
        "                print(\"Now patchifying image:\", path+\"/\"+image_name)\n",
        "                patches_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n",
        "        \n",
        "                for i in range(patches_img.shape[0]):\n",
        "                    for j in range(patches_img.shape[1]):\n",
        "                        \n",
        "                        single_patch_img = patches_img[i,j,:,:]\n",
        "                        \n",
        "                        #Use minmaxscaler instead of just dividing by 255. \n",
        "                        single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n",
        "                        \n",
        "                        #single_patch_img = (single_patch_img.astype('float32')) / 255. \n",
        "                        single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
        "                        image_dataset.append(single_patch_img)"
      ],
      "metadata": {
        "id": "oM1gOxSZ2pMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#patchifying masks\n",
        "mask_dataset = []  \n",
        "for path, subdirs, files in os.walk(root_directory):\n",
        "    #print(path)  \n",
        "    dirname = path.split(os.path.sep)[-1]\n",
        "    if dirname == 'masks':   #Find all 'images' directories\n",
        "        masks = os.listdir(path)  #List of all image names in this subdirectory\n",
        "        for i, mask_name in enumerate(masks):  \n",
        "            if mask_name.endswith(\".png\"):   #Only read png images... (masks in this dataset)\n",
        "               \n",
        "                mask = cv2.imread(path+\"/\"+mask_name, 1)  #Read each image as Grey (or color but remember to map each color to an integer)\n",
        "                mask = cv2.cvtColor(mask,cv2.COLOR_BGR2RGB)\n",
        "                SIZE_X = (mask.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
        "                SIZE_Y = (mask.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
        "                mask = Image.fromarray(mask)\n",
        "                mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
        "                #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
        "                mask = np.array(mask)             \n",
        "       \n",
        "                #Extract patches from each image\n",
        "                print(\"Now patchifying mask:\", path+\"/\"+mask_name)\n",
        "                patches_mask = patchify(mask, (patch_size, patch_size, 3), step=patch_size)  #Step=256 for 256 patches means no overlap\n",
        "        \n",
        "                for i in range(patches_mask.shape[0]):\n",
        "                    for j in range(patches_mask.shape[1]):\n",
        "                        \n",
        "                        single_patch_mask = patches_mask[i,j,:,:]\n",
        "                        #single_patch_img = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n",
        "                        single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
        "                        mask_dataset.append(single_patch_mask) \n",
        " \n",
        "image_dataset = np.array(image_dataset)\n",
        "mask_dataset =  np.array(mask_dataset)"
      ],
      "metadata": {
        "id": "Aux23k6w2pv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################################################\n",
        "\"\"\"\n",
        "RGB to HEX: (Hexadecimel --> base 16)\n",
        "This number divided by sixteen (integer division; ignoring any remainder) gives \n",
        "the first hexadecimal digit (between 0 and F, where the letters A to F represent \n",
        "the numbers 10 to 15). The remainder gives the second hexadecimal digit. \n",
        "0-9 --> 0-9\n",
        "10-15 --> A-F\n",
        "Example: RGB --> R=201, G=, B=\n",
        "R = 201/16 = 12 with remainder of 9. So hex code for R is C9 (remember C=12)\n",
        "Calculating RGB from HEX: #3C1098\n",
        "3C = 3*16 + 12 = 60\n",
        "10 = 1*16 + 0 = 16\n",
        "98 = 9*16 + 8 = 152\n",
        "\"\"\"\n",
        "#Convert HEX to RGB array\n",
        "# Try the following to understand how python handles hex values...\n",
        "a=int('3C', 16)  #3C with base 16. Should return 60. \n",
        "print(a)\n",
        "#Do the same for all RGB channels in each hex code to convert to RGB\n",
        "Building = '#3C1098'.lstrip('#')\n",
        "Building = np.array(tuple(int(Building[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152\n",
        "\n",
        "Land = '#8429F6'.lstrip('#')\n",
        "Land = np.array(tuple(int(Land[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246\n",
        "\n",
        "Road = '#6EC1E4'.lstrip('#') \n",
        "Road = np.array(tuple(int(Road[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228\n",
        "\n",
        "Vegetation =  'FEDD3A'.lstrip('#') \n",
        "Vegetation = np.array(tuple(int(Vegetation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58\n",
        "\n",
        "Water = 'E2A929'.lstrip('#') \n",
        "Water = np.array(tuple(int(Water[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41\n",
        "\n",
        "Unlabeled = '#9B9B9B'.lstrip('#') \n",
        "Unlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) #155, 155, 155\n",
        "\n",
        "label = single_patch_mask\n",
        "\n",
        "# Now replace RGB to integer values to be used as labels.\n",
        "#Find pixels with combination of RGB for the above defined arrays...\n",
        "#if matches then replace all values in that pixel with a specific integer\n",
        "def rgb_to_2D_label(label):\n",
        "    \"\"\"\n",
        "    Suply our labale masks as input in RGB format. \n",
        "    Replace pixels with specific RGB values ...\n",
        "    \"\"\"\n",
        "    label_seg = np.zeros(label.shape,dtype=np.uint8)\n",
        "    label_seg [np.all(label == Building,axis=-1)] = 0\n",
        "    label_seg [np.all(label==Land,axis=-1)] = 1\n",
        "    label_seg [np.all(label==Road,axis=-1)] = 2\n",
        "    label_seg [np.all(label==Vegetation,axis=-1)] = 3\n",
        "    label_seg [np.all(label==Water,axis=-1)] = 4\n",
        "    label_seg [np.all(label==Unlabeled,axis=-1)] = 5\n",
        "    \n",
        "    label_seg = label_seg[:,:,0]  #Just take the first channel, no need for all 3 channels\n",
        "    \n",
        "    return label_seg\n",
        "\n",
        "labels = []\n",
        "for i in range(mask_dataset.shape[0]):\n",
        "    label = rgb_to_2D_label(mask_dataset[i])\n",
        "    labels.append(label)    \n",
        "\n",
        "labels = np.array(labels)   \n",
        "labels = np.expand_dims(labels, axis=3)"
      ],
      "metadata": {
        "id": "TXR-StLw2sN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_classes = len(np.unique(labels))\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "labels_cat = to_categorical(labels, num_classes=n_classes)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_dataset, labels_cat, test_size = 0.20, random_state = 42)"
      ],
      "metadata": {
        "id": "h6TnLS3b2u9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(10))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "txykFSo_2w4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import keras_tuner as kt\n",
        "tuner = kt.Hyperband( model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')"
      ],
      "metadata": {
        "id": "nQQHd-Hk2zJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'learning_rate': 0.001,\n",
        "    'activation_type': 'relu',\n",
        "    'batch_size': 16\n",
        "}\n"
      ],
      "metadata": {
        "id": "kFQkyfa421Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nni\n",
        "import nni\n",
        "optimized_params = nni.get_next_parameter()\n",
        "params.update(optimized_params)\n",
        "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
        "dice_loss = sm.losses.DiceLoss(class_weights=weights) \n",
        "focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "total_loss = dice_loss + (1 * focal_loss)\n",
        "IMG_HEIGHT = X_train.shape[1]\n",
        "IMG_WIDTH  = X_train.shape[2]\n",
        "IMG_CHANNELS = X_train.shape[3]"
      ],
      "metadata": {
        "id": "oglKok2324MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "metrics=['accuracy', jacard_coef]\n",
        "\n",
        "def multi_unet_model(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=1, activation_type='relu'):\n",
        "#Build the model\n",
        "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
        "    s = inputs\n",
        "\n",
        "    #Contraction path\n",
        "    c1 = Conv2D(16, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(s)\n",
        "    c1 = Dropout(0.2)(c1)  # Original 0.1\n",
        "    c1 = Conv2D(16, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "    \n",
        "    c2 = Conv2D(32, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = Dropout(0.2)(c2)  # Original 0.1\n",
        "    c2 = Conv2D(32, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "     \n",
        "    c3 = Conv2D(64, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "     \n",
        "    c4 = Conv2D(128, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "     \n",
        "    c5 = Conv2D(256, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "    c5 = Conv2D(256, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c5)\n",
        "    \n",
        "    #Expansive path \n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c6)\n",
        "     \n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = Dropout(0.2)(c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c7)\n",
        "     \n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = Dropout(0.2)(c8)  # Original 0.1\n",
        "    c8 = Conv2D(32, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c8)\n",
        "     \n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = Dropout(0.2)(c9)  # Original 0.1\n",
        "    c9 = Conv2D(16, (3, 3), activation=activation_type, kernel_initializer='he_normal', padding='same')(c9)\n",
        "     \n",
        "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
        "     \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    \n",
        "    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n",
        "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    #model.summary()\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "3XyLKH_129L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resource used to help implement this section of the code \"https://nni.readthedocs.io/en/stable/tutorials/hpo_quickstart_tensorflow/model.html\" and \"https://nni.readthedocs.io/en/stable/hpo/tuners.html\"\n",
        "\n",
        "model = multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS, activation_type=params['activation_type'])\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
        "model.compile(optimizer=adam, loss=total_loss, metrics='accuracy')\n",
        "callback = tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_end = lambda epoch, logs: nni.report_intermediate_result(logs['accuracy'])\n",
        ")\n"
      ],
      "metadata": {
        "id": "BAanSTV929uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history1 = model.fit(X_train, y_train, batch_size=params['batch_size'], epochs=80, validation_data=(X_test, y_test), callbacks=[callback])\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "nni.report_final_result(accuracy)"
      ],
      "metadata": {
        "id": "WD_pVKiJ3Ahd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training and validation accuracy and loss at each epoch\n",
        "history = history1\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8yMe58J43DAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(X_test)\n",
        "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
        "y_test_argmax=np.argmax(y_test, axis=3)"
      ],
      "metadata": {
        "id": "plaP9In63HQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import MeanIoU\n",
        "n_classes = 6\n",
        "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
        "IOU_keras.update_state(y_test_argmax, y_pred_argmax)\n",
        "print(\"Mean IoU =\", IOU_keras.result().numpy())"
      ],
      "metadata": {
        "id": "SUpJOuro3Jik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (15):\n",
        "  test_img_number = random.randint(0, len(X_test))\n",
        "  test_img = X_test[test_img_number]\n",
        "  ground_truth=y_test_argmax[test_img_number]\n",
        "\n",
        "  test_img_input=np.expand_dims(test_img, 0)\n",
        "  prediction = (model.predict(test_img_input))\n",
        "  predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plt.subplot(231)\n",
        "  plt.title('Testing Image')\n",
        "  plt.imshow(test_img)\n",
        "  plt.subplot(232)\n",
        "  plt.title('Testing Label')\n",
        "  plt.imshow(ground_truth)\n",
        "  plt.subplot(233)\n",
        "  plt.title('Prediction image')\n",
        "  plt.imshow(predicted_img)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "agI486E63LTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "predictionVal = np.array(y_pred_argmax)\n",
        "predictionVal = np.stack(predictionVal, axis=1).flatten()\n",
        "trueVal = np.array(y_test_argmax)\n",
        "trueVal = np.stack(trueVal, axis=1).flatten()\n",
        "yTest = predictionVal.tolist()\n",
        "xTest = trueVal.tolist()\n",
        "labelVal = np.unique(labels).tolist()\n",
        "cmVal = confusion_matrix(xTest, yTest, labels=labelVal)\n",
        "precisionVal = precision_score(xTest, yTest, average='micro')\n",
        "recallVal = recall_score(xTest, yTest, average='micro')\n",
        "print(\"This is the calculated Precision Value\", precisionVal , \"and this is the calculated Recall Value\", recallVal)\n",
        "\n",
        "precisionVal, recallVal, thresholds = precision_recall_curve(xTest, yTest, pos_label=1)\n",
        "plt.plot(precisionVal, recallVal)\n",
        "plt.xlabel('Precision')\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.title(\"precision_recall_curve\")\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "zitFDp1e30sO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}